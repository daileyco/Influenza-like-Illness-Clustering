---
title: "Reproducibility Notebook"
output: html_notebook
---

# Project Overview

This repository / collection of scripts is a component of Cody Dailey's dissertation research. Altogether, the scope of this dissertation research spans the spatial epidemiology and evolution of seasonal influenza in the United States from 2010 to 2020. Here, I focus on an initial exploration of influenza-like illness incidence and seasonal epidemic intensity.   
  
> How do you eat an elephant?  
>   
>> One bite at a time.  
  
## Document Overview

This document is my "reproducibility notebook", basically, a data analyst's version of a lab notebook. It contains (hopefully) an easy-to-follow description of data management and analyses with associated scripts. My goal with this document is to ensure reproducibility in my research. Below, I will describe necessary steps in my workflow while sourcing various scripts to implement / execute this workflow. Note that this is supplemental information to the traditional scientific manuscript (which is found in another document) and will include much more technical details and analytical decisions / commentary. 



# Objective

* To identify spatial and network clustering patterns in influenza-like illness in the US from 2011 to 2020


# Approach


## Data Processing / Initial Cleaning



### Influenza-like Illness
ILI data from FluView and Florida Department of Public Health were processed using following script:

Data were stored in a dataframe with each row corresponding to a specific location (state) during a specific week (epi week and date) with the reported counts of ILI and total patients for that location during that week. 

Data were originally split for New York City and New York; I summed those to a single location. Also, data from FluView are missing Florida data completely, but LD was able to obtain data from the Florida Department of Public Health. The data greatly overlap, but the Florida data seems to be missing some of the earliest and latest data (effectively trimming the dataset from both timepoints ends). 

```{r}
source("./02-Scripts/01-Data-Wrangling/process_Data_ILI.R")
```

```{r}
source("./02-Scripts/01-Data-Wrangling/process_Data_ILI_Rates_of_Change.R")
```



### Mobility Data

Worker commuting flows from the American Community Survey for both 2011-2015 and 2016-2020 are downloaded as excel spreadsheets, though not in a tidy data format (e.g., there is extra fluff at the top and bottom of the excel tables). The ACS data essentially contain four main variables: a location of residence, a location of work, a point estimate of the number of workers commuting between the two locations, and an estimate of uncertainty (i.e., margin of error) for the point estimate. These data are aggregated to the state level to align with the ILI data. In aggregating the data to the state level, the measure of uncertainty is lost; is there a simple way to account for each margin of error in aggregate data? For now, we will focus on the locations and the point estimate and potentially return to the uncertainty later for sensitivity analyses.   
  
The process script is simple enough as it just loads the data, categorizes the type of commuter flow (intracounty, intrastate, interstate, international), aggregates totals to the state level, combines the data from both time ranges, and then saves an .rds file. Additionally, the data are expanded to have entries for all possible combinations of locations; this allows me to note the "unobserved" flows.  

The script is sourced using the following code:
```{r}
source("./02-Scripts/01-Data-Wrangling/process_Data_ACS.R")
```

### Spatial Data

Next, we process some spatial data: shapefiles for US counties and states, coordinates for population centers for counties and states

```{r}
source("./02-Scripts/01-Data-Wrangling/process_Data_Spatial.R", echo = T)
```











## Data Management & Analysis Setup


### Writing SaTScan Data Input Files

Next, we need to write the files for input in SaTScan. Basically a coordinates file used in spatial scan statistics (location-label lat lon) (going to revisit this as you can supply a network with pairwise distances, inverted commuter flow?); a "case" file, here the proportions of cumulative incidence (square root transformed) OR ("2" suffix) the sub-component of shannon entropy (sqrt(p*log(p)); also square root transformed as indicated). The method assumes a normally distributed continuous variable. 

In addition to a "normally" distributed "case" characteristic, each value needs a weight assigned to it so that the underlying logic/rationale / variance estimation understands that the given value is an "average" for the entire region (or data sample). I found in the help documentation for satscan that a population can be used instead of a weight. So, at first I thought to use the cumulative total patients (the weekly denominator for ILI data). However, I'm also think that it may be better to use the cumulative total patients count for a single location season for each week in that season. But im torn. Variance in proportion of cumulative ILI may depend on ... proportion of observed patients at that location during that week. I don't think it should be the raw counts for either ILI or total patients, the whole proportion calculation is a work around to known bias. So, we'll just go with total patients for now. 



Case file has (location-label week value value-weight). So, an individual case file is written for each week; the coordinates file is set once. The text files are written in the following scripts:



```{r}
source("./02-Scripts/01-Data-Wrangling/prep_Files_SaTScan_ILI.R")
```




```{r}
source("./02-Scripts/01-Data-Wrangling/prep_Files_SaTScan_Adjacency.R")
```




### SaTScan Parameters

Intermediate step, manually create a parameters file for satscan to read. I think `satscan` package helps with that but I just used gui to save file after manually selecting options. 



## Analysis - SaTScan 

Next, I wrote scripts to send commands to the command prompt to run the satscan batch mode (executable file? idk how to say). The only option needed there is a parameters file indicating the input files (coordinates, cases, ...) and the model options. 

The script loops over list of dates and replaces the input file string in the parameters file with each run. So, the program does a purely spatial clustering analysis assuming a continuously distributed outcome variable. 

```{r}
source("./02-Scripts/04-Analysis/run_Program_SaTScan.R")
```


Each SaTScan run generates an output text file (can do shapefiles and maybe R package readable files too, but who likes easy integration?). The text files contain information about each detected cluster (if any as I had to debug). 





## SaTScan Results Processing

The following script reads the output text files and parse the data for each cluster into a dataframe:


```{r}
source("./02-Scripts/01-Data-Wrangling/process_Results_SaTScan.R")
```

These will be transferred to another repo for a secondary analysis. 



## Visualizations

```{r}
source("./02-Scripts/03-Visualization/generate_Table_Summaries.R")
```


```{r}
source("./02-Scripts/03-Visualization/generate_Flextable_Summaries.R")
```


```{r}
source("./02-Scripts/03-Visualization/generate_Figures_Clusters_over_Time.R")
```


```{r}
source("./02-Scripts/03-Visualization/generate_Figures_Cluster_Composites.R")
```





































